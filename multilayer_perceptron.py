# -*- coding: utf-8 -*-
"""Multilayer_Perceptron (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bkqP92phU22tCI7kzanNdlYsCX7Qoi8J

<ins><b>Neural Network & Deep Learning </b></ins>




Multilayer Perceptron and Hyperparameter Tuning

# Question

- Experiment the performance of the Multilayer Perceptron on the dataset used for Perceptron Implementation.
- Moreover, tune a Multilayer Perceptron on the Adult Dataset.

# Importing the required modules
"""

# Modules used for data handling and linear algebra operations.
import pandas as pd
import numpy as np

# Modules used for data visualization.
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style()

# Modules used for preprocessing
from sklearn.preprocessing import OneHotEncoder

# Modules used for Machine Learning models.
from sklearn.linear_model import Perceptron
from sklearn.neural_network import MLPClassifier

# Modules used for hyperparameter tuning.
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

# Models used for evaluating the model.
from sklearn import metrics
from sklearn.model_selection import cross_val_score

# Suppressing the warnings.
import warnings
warnings.filterwarnings('ignore')

"""# Comparision of the Perceptron and MLP on Credit Card Approval Dataset

## Reading the dataset
"""

df = pd.read_csv("credit-approval.data", header=None)

"""## Glimpse into the dataset"""

df.head()

df.info()

"""## Data Type Distribution"""

df.dtypes.value_counts().plot(kind="bar",
                              title="Types of Data",
                              xlabel="Data Type",
                              ylabel="No.of columns",
                              rot=0,
                              color=["crimson","orange"])
plt.show()

"""Most of columns are categorical while the others are numerical.

## Seggragation of Columns
"""

cat_cols = []
num_cols = []

for i in df.columns:
    if df[i].dtype == "O":
        cat_cols.append(i)
    else:
        num_cols.append(i)

"""## Missing Values Analysis"""

null_freq = []
for i in df.columns:
    f = dict(df[i].value_counts())
    if "?" in f.keys():
        null_freq.append(f["?"]*100/len(df))
    else:
        null_freq.append(0)

pd.Series(dict(zip(df.columns,null_freq))).plot(kind="bar",
                                                rot=0,
                                                title="Missing Value Frequency",
                                                xlabel="Column Name",
                                                ylabel="Percentage of missing values",
                                                color=["orange","crimson"])
plt.show()

"""Only 2% of the rows are null and hence directly dropping them is feasible compared to imputation.

## Target Variable Analysis
"""

df[15].value_counts().plot(kind="bar",
                           title="Class Distribution",
                           xlabel="Status of Credit Card Approval",
                           ylabel="Frequency of the Status",
                           color=["crimson","orange"],
                           rot=0)
plt.show()

"""The dataset is biased to class "-" since it has higher instances of that class compared to others.

## Pre-processing

### Handling Missing Values
"""

df = df.replace({"?":None})

df = df.dropna()

"""### Encoding the categorical variables"""

encoder = OneHotEncoder(sparse=False)
for i in cat_cols:
    df[i] = encoder.fit_transform(df[i].values.reshape(-1,1))

df = df.reset_index()

"""## Train-Test Split"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(df,
    df[15],
    test_size = 0.10,
    train_size=0.90,
    random_state = 0
)
X_train.pop(15)
X_test.pop(15)

"""## Perceptron

### Training and Fitting the model
"""

clf = Perceptron(random_state=0)

clf.fit(X_train,y_train)

"""### Validating the model

#### Predictions from the model on the train and test dataset
"""

y_pred_train = clf.predict(X_train)
y_pred_test = clf.predict(X_test)

"""#### Accuracy on the train dataset"""

metrics.accuracy_score(y_true=y_train,y_pred=y_pred_train)

"""#### Accuracy on the test dataset"""

metrics.accuracy_score(y_true=y_test,y_pred=y_pred_test)

"""## Multilayer Perceptron"""

clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)

"""### Validating the model

#### Predictions from the model
"""

y_pred_train = clf.predict(X_train)
y_pred_test = clf.predict(X_test)

"""#### Accuracy on the train dataset"""

metrics.accuracy_score(y_true=y_train,y_pred=y_pred_train)

"""#### Accuracy on the test dataset"""

metrics.accuracy_score(y_true=y_test,y_pred=y_pred_test)

"""## Comparative Inferences

- The Perceptron model gives an accuracy of 67% and 75% on the test and train dataset respectively.
- The MLP model gives an accuracy of 78% and 79% on the test and train dataset respectively.
- Addition of hidden layers gives better results indicating the dataset is not linearly separable.
- The MLP is able to learn a non-linear decision boundary compared to Perceptron which is a linear classifier.
"""